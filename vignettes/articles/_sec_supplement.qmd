## Details of Ordinal SuStaIn {.appendix}

The Ordinal SuStaIn approach [@young2021ordinal] uses a scored events model, 
which assumes that for each `r outcomes_name`,
there are a discrete set of 
underlying ordinal severity levels,
but the measured versions of the `r outcomes_name`s may contain 
some amount of random noise.
For example, an individual who is really at Ataxia Severity level 2 may be 
incorrectly assessed as being at Ataxia Severity level 1, depending on
the patient's temporary disease status on the day of the exam or
inter-rater differences.

The first step in applying the Ordinal SuStaIn algorithm is to determine,
for each `r outcomes_name`, the probability that an individual is 
"correctly scored" at their "true underlying level".
In this analysis, we assumed that all controls are truly at the reference levels
for each `r outcomes_name`, 
and we estimated the probability of correct scoring as 
the proportion of controls 
who were assessed as being at the reference level.
It is crucial to allow some possibility of incorrect scoring, 
so we capped the estimated correct scoring probabilities at 
`r max_prob_correct *100`% (@tbl-prob-correct).

```{r}
#| label: tbl-prob-correct
#| tbl-cap: |
#|   **Proportions of controls at baseline levels, and 
#|   corresponding estimates of probability of correct scoring.**s

prob_correct |> attr("data")
```

## Imputation of missing data {.appendix}

@tbl-ex-incomplete-data shows the distribution of Ataxia severity among
study participants with Fragile X premutation (55-199 CGG repeats),
when Ataxia severity was recorded.



```{r}
#| label: tbl-ex-incomplete-data
#| tbl-cap: "**Distribution of ataxia severity among cases.**"

table1(
  na.is.category = FALSE,
  data = v1_usable_cases,
  x = ~ `Ataxia: severity*`)
```

{{< pagebreak >}}

{{< include general-data-notes.qmd >}}

{{< pagebreak >}}

## Comparing CGG levels stratified by sex {.appendix #sec-cgg-strat-sex}

```{r}
#| label: fig-pvd-by-cgg-male
#| column: page
#| fig-height: !expr pvd_height
#| fig-width: !expr pvd_width
#| fig-cap: !expr glue::glue("**Event sequences stratified by CGG repeats (<100 vs 100+), males only. (a)** {compact_fig_caption} **(b)** Positional differences in estimated event sequence between CGG repeat levels. {compact_fig_cap2}")
#| layout-ncol: 1
#| fig-subcap:
#|    - ""
#|    - ""
#| fig-cap-location: top

# compact pvd
list(
    "Males, CGG &lt; 100 <br>" = fig_under100_males,
    "Males, CGG &ge; 100  <br>" = fig_over100_males
  ) |>
plot_compact_pvd(y_text_size = 11)

# lineplot of event sequences
list(
    "Males, CGG &lt; 100" = fig_under100_males,
    "Males, CGG &ge; 100" = fig_over100_males
  ) |>
pvd_lineplot(y_text_size = 11)
```

{{< pagebreak >}}

```{r}
#| label: fig-pvd-by-cgg-female
#| column: page
#| fig-height: !expr pvd_height
#| fig-width: !expr pvd_width
#| fig-cap: !expr
#|    glue::glue(
#|      "**Event sequences stratified by CGG repeats among females.**
#|      **(a)** Estimated event sequences,
#|      stratified by CGG repeats (<100 vs 100+).
#|      {compact_fig_caption}
#|      **(b)** Positional differences in estimated event sequence
#|      between CGG repeats. {compact_fig_cap2}")
#| layout-ncol: 1
#| fig-subcap:
#|    - ""
#|    - ""
#| fig-cap-location: top

# compact pvd
plot_compact_pvd(
  figs = list(
    fig_under100_females,
    fig_over100_females
  ),
  facet_label = c("Females, CGG &lt; 100  <br>", "Females, CGG &ge; 100  <br>"),
  y_text_size = 11
)

# lineplot of event sequences
pvd_lineplot(
  figs = list(
    fig_under100_females,
    fig_over100_females
  ),
  facet_label = c("Females, CGG &lt; 100", "Females, CGG &ge; 100"),
  y_text_size = 11
)
```

{{< pagebreak >}}

## Hypothesis testing {.appendix #sec-permutation-test}

```{r}
#| label: hypothesis-tests
obs_stat = pval_sex_gp34_v1  |> attr("observed_test_stat")
perm_stats = pval_sex_gp34_v1  |> attr("permuted_test_stats")
n_perm_over_obs = sum(perm_stats >= obs_stat)
pct_perm_over_obs = mean(perm_stats >= obs_stat)
```

@fig-permute-llik-sex illustrates the permutation test approach that we implemented to compare event orderings between subgroups.
This figure compares the mean log-likelihood for the sex-stratified model
to the distribution of mean log-likelihoods when sex is permuted.
Only `r n_perm_over_obs` out of `r length(perm_stats)` permuted mean-log-likelihoods
were larger than the observed mean-log-likelihood
(`r pct_perm_over_obs |> scales::percent(accuracy = 0.1)`),
resulting in a two-sided p-value of $2 \times `r pct_perm_over_obs` = `r 2*pct_perm_over_obs`$.

{{< pagebreak >}}

```{r}
#| fig-cap: |
#|     **Histogram of mean log-likelihoods for data permuted by sex.**
#| label: fig-permute-llik-sex

pval_sex_gp34_v1 |> autoplot() |> print()
```

{{< pagebreak >}}

<!-- ## Results with repeated measures {#sec-longitudinal-results .appendix} -->

<!-- {{< include longitudinal-results.qmd >}} -->

## Detecting latent subtypes {#sec-subtypes .appendix}

In order to use the Ordinal SuStaIn modeling algorithm, we must specify how many latent subtypes to include in the model. There are several metrics for determining the optimal number of subtypes for a given data set.

### Likelihood {.appendix}

The simplest option is to compare the likelihood of the data for the fitted model while varying the number of subtypes used to fit the model.
@fig-mcmc-loglik shows the distribution of log-likelihoods from the MCMC samples for the full dataset (not stratified by sex or CGG repeats).
Adding up to 6 clusters substantially improves the log-likelihood.

```{r}
#| fig-height: !expr pvd_height
#| fig-width: !expr pvd_width
#| label: fig-mcmc-loglik
#| fig-cap: log-likelihoods of MCMC samples, by number of subtypes
#| eval: !expr fit_models

lliks = results_v1 |>
  sapply(F = function(x) x$samples_likelihood)

lliks |>
  graph_likelihoods_v2(alpha = 0.5) |>
  suppressWarnings()

```

{{< pagebreak >}}

### Cross-Validation Information Criterion {#sec-cvic .appendix}

Since models with fewer subtypes are limited special cases of models with more subtypes
(i.e., "nested models"), the likelihood of the training data is guaranteed to improve
as we increase the number of subtypes.
However, using too many subtypes means there is less data available for each subtype,
possibly leading to a model that does not generalize well to new data
(i.e., a model that is "overfit" to the training data).

To avoid overfitting,
Young et al [-@young2018uncovering] recommends
choosing the optimal number
using the Cross-Validation Information Criterion (CVIC) (@eq-def-cvic).

Let:

* $n$ be the number of observations in the dataset
* $Y_i$ be the observed biomarker values for the $i^{th}$ participant
* $\mathcal C$ be the number of latent subtypes assumed, with corresponding index $c$.
* $C_i$ be the latent subtype membership of observation $i$
* $\mathcal S$ be the set of possible event sequences, indexed by $\mathcal s$
* $S_c \in \mathcal S$ be the sequence for subtype $c$
* $K$ be the number of cross-validation folds, with index $k$; typically, $K = 10$.
* $n_k$ be the number of held-out observations in cross-validation fold $K$; $n_k \approx n/K$
* $\hat{P}_{(-k)}(C = c)$ be the prior probability that an observation belongs to subtype $c$, estimated using all observations except those in fold $k$.

Then the Cross-Validation Information Criterion (CVIC) is defined as:

$$
CVIC_{\mathcal C} =
-2 *
\sum_{k=1}^K
\sum_{i=1}^{n_k}
\log
\left\{
\sum_{c=1}^\mathcal C
\sum_{s\in \mathcal S}
\hat P(Y_i|C_i = c, S_c=s)
\hat{P}_{(-k)}(C_i = c, S_c = s)
\right\}
$${#eq-def-cvic}

### Out-of-fold log-likelihood criterion {#sec-oofll .appendix}

We also evaluated the consistency of our cross-validation procedure
by looking at the distribution of out-of-fold log-likelihood ("OOFLL")
across cross-validation folds;
following Young et al [-@young2018uncovering],
we calculated out-of-sample log-likelihood as:

$$
\ell_{\mathcal C, k} =
\sum_{i = 1}^{n_k}
\sum_{c=1}^\mathcal C
\sum_{s\in \mathcal S}
\log
\left\{
\hat P(Y_i|C_i = c, S_c=s)
\right\}
\hat{P}_{(-k)}(C_i = c, S_c = s)
$$ {#eq-def-oof-llik}

@fig-cvic-2 shows the distribution of the OOFLL statistic
as a function of number of latent subgroups.

@fig-stage-by-age shows estimated disease progression stage
(that is, number of events experienced)
by age at visit and estimated latent subtype,
with a
Locally Estimated Scatterplot Smoothing (LOESS)
non-parametric regression curve superimposed [@cleveland1992local].
This graph is a model diagnostic; if the model fits the data well,
then older patients should on average be classified as being farther
along in their disease progression, resulting in an upwards trend.
Types 1, 3 and 4 appear to show such an upwards trend.
Type 2 has a less clear trend, but does not show substantial evidence
of lack of fit.
In all four subtypes, patients whose visit occurred at a later age
tend to be at a later estimated stage of disease progression.
The upwards trend of progression stage with age
indicates that the estimated models are plausible.

```{r}
#| label: fig-stage-by-age
#| fig-cap: |
#|    **Estimated progression stage by age and latent subtype.**
#|    The blue line is a Locally Estimated Scatterplot Smoothing (LOESS)
#|    non-parametric regression curve.
#|
#| include: true

stages =
  results_cv_max$subtype_and_stage_table |>
  mutate(
    age = patient_data$`Age at visit`,
    id = patient_data$`FXS ID`
  )

library(ggplot2)
stages |>
  graph_stage_by_age()

```

::: landscape

```{r}
#| fig-cap: "Selection criteria for number of latent subtypes"
#| fig-subcap:
#|    - "Cross-validation information criterion"
#|    - "Test set log-likelihood across folds"
#| label: fig-cvic
#| layout-ncol: 2
#| eval: !expr fit_models

temp$CVIC |> plot_CVIC()
temp$loglike_matrix |> plot_cv_loglik()
```

:::

## Additional data tables {.appendix}

The following tables summarize the distributions of the `r outcomes_name`s
that we analyzed in this paper, stratified by sex and CGG level.

::: landscape

{{< include exploration.qmd >}}

:::

## Imputation of missing data {.appendix}

@tbl-ex-incomplete-data shows the distribution of Ataxia severity among
study participants with Fragile X premutation (55-199 CGG repeats),
when Ataxia severity was recorded.



```{r}
#| label: tbl-ex-incomplete-data
#| tbl-cap: "**Distribution of ataxia severity among cases.**"

table1(
  na.is.category = FALSE,
  data = v1_usable_cases,
  x = ~ `Ataxia: severity*`)
```

{{< pagebreak >}}

{{< include general-data-notes.qmd >}}

{{< pagebreak >}}

## Comparing CGG levels stratified by sex {.appendix #sec-cgg-strat-sex}

```{r}
#| label: fig-pvd-by-cgg-male
#| column: page
#| fig-height: !expr pvd_height
#| fig-width: !expr pvd_width
#| fig-cap: !expr glue::glue("**Event sequences stratified by CGG repeats (<100 vs 100+), males only. (a)** {compact_fig_caption} **(b)** Positional differences in estimated event sequence between CGG repeat levels. {compact_fig_cap2}")
#| layout-ncol: 1
#| fig-subcap:
#|    - ""
#|    - ""
#| fig-cap-location: top

# compact pvd
list(
    "Males, CGG &lt; 100 <br>" = fig_under100_males,
    "Males, CGG &ge; 100  <br>" = fig_over100_males
  ) |>
plot_compact_pvd(y_text_size = 11)

# lineplot of event sequences
list(
    "Males, CGG &lt; 100" = fig_under100_males,
    "Males, CGG &ge; 100" = fig_over100_males
  ) |>
pvd_lineplot(y_text_size = 11)
```

{{< pagebreak >}}

```{r}
#| label: fig-pvd-by-cgg-female
#| column: page
#| fig-height: !expr pvd_height
#| fig-width: !expr pvd_width
#| fig-cap: !expr
#|    glue::glue(
#|      "**Event sequences stratified by CGG repeats among females.**
#|      **(a)** Estimated event sequences,
#|      stratified by CGG repeats (<100 vs 100+).
#|      {compact_fig_caption}
#|      **(b)** Positional differences in estimated event sequence
#|      between CGG repeats. {compact_fig_cap2}")
#| layout-ncol: 1
#| fig-subcap:
#|    - ""
#|    - ""
#| fig-cap-location: top

# compact pvd
plot_compact_pvd(
  figs = list(
    fig_under100_females,
    fig_over100_females
  ),
  facet_label = c("Females, CGG &lt; 100  <br>", "Females, CGG &ge; 100  <br>"),
  y_text_size = 11
)

# lineplot of event sequences
pvd_lineplot(
  figs = list(
    fig_under100_females,
    fig_over100_females
  ),
  facet_label = c("Females, CGG &lt; 100", "Females, CGG &ge; 100"),
  y_text_size = 11
)
```

{{< pagebreak >}}

## Hypothesis testing {.appendix #sec-permutation-test}

```{r}
#| label: hypothesis-tests
obs_stat = pval_sex_gp34_v1  |> attr("observed_test_stat")
perm_stats = pval_sex_gp34_v1  |> attr("permuted_test_stats")
n_perm_over_obs = sum(perm_stats >= obs_stat)
pct_perm_over_obs = mean(perm_stats >= obs_stat)
```

@fig-permute-llik-sex illustrates the permutation test approach that we implemented to compare event orderings between subgroups.
This figure compares the mean log-likelihood for the sex-stratified model
to the distribution of mean log-likelihoods when sex is permuted.
Only `r n_perm_over_obs` out of `r length(perm_stats)` permuted mean-log-likelihoods
were larger than the observed mean-log-likelihood
(`r pct_perm_over_obs |> scales::percent(accuracy = 0.1)`),
resulting in a two-sided p-value of $2 \times `r pct_perm_over_obs` = `r 2*pct_perm_over_obs`$.

{{< pagebreak >}}

```{r}
#| fig-cap: |
#|     **Histogram of mean log-likelihoods for data permuted by sex.**
#| label: fig-permute-llik-sex

pval_sex_gp34_v1 |> autoplot() |> print()
```

{{< pagebreak >}}

<!-- ## Results with repeated measures {#sec-longitudinal-results .appendix} -->

<!-- {{< include longitudinal-results.qmd >}} -->

## Detecting latent subtypes {#sec-subtypes .appendix}

In order to use the Ordinal SuStaIn modeling algorithm, we must specify how many latent subtypes to include in the model. There are several metrics for determining the optimal number of subtypes for a given data set.

### Likelihood {.appendix}

The simplest option is to compare the likelihood of the data for the fitted model while varying the number of subtypes used to fit the model.
@fig-mcmc-loglik shows the distribution of log-likelihoods from the MCMC samples for the full dataset (not stratified by sex or CGG repeats).
Adding up to 6 clusters substantially improves the log-likelihood.

```{r}
#| fig-height: !expr pvd_height
#| fig-width: !expr pvd_width
#| label: fig-mcmc-loglik
#| fig-cap: log-likelihoods of MCMC samples, by number of subtypes
#| eval: !expr fit_models

lliks = results_v1 |>
  sapply(F = function(x) x$samples_likelihood)

lliks |>
  graph_likelihoods_v2(alpha = 0.5) |>
  suppressWarnings()

```

{{< pagebreak >}}

### Cross-Validation Information Criterion {#sec-cvic .appendix}

Since models with fewer subtypes are limited special cases of models with more subtypes
(i.e., "nested models"), the likelihood of the training data is guaranteed to improve
as we increase the number of subtypes.
However, using too many subtypes means there is less data available for each subtype,
possibly leading to a model that does not generalize well to new data
(i.e., a model that is "overfit" to the training data).

To avoid overfitting,
Young et al [-@young2018uncovering] recommends
choosing the optimal number
using the Cross-Validation Information Criterion (CVIC) (@eq-def-cvic).

Let:

* $n$ be the number of observations in the dataset
* $Y_i$ be the observed biomarker values for the $i^{th}$ participant
* $\mathcal C$ be the number of latent subtypes assumed, with corresponding index $c$.
* $C_i$ be the latent subtype membership of observation $i$
* $\mathcal S$ be the set of possible event sequences, indexed by $\mathcal s$
* $S_c \in \mathcal S$ be the sequence for subtype $c$
* $K$ be the number of cross-validation folds, with index $k$; typically, $K = 10$.
* $n_k$ be the number of held-out observations in cross-validation fold $K$; $n_k \approx n/K$
* $\hat{P}_{(-k)}(C = c)$ be the prior probability that an observation belongs to subtype $c$, estimated using all observations except those in fold $k$.

Then the Cross-Validation Information Criterion (CVIC) is defined as:

$$
CVIC_{\mathcal C} =
-2 *
\sum_{k=1}^K
\sum_{i=1}^{n_k}
\log
\left\{
\sum_{c=1}^\mathcal C
\sum_{s\in \mathcal S}
\hat P(Y_i|C_i = c, S_c=s)
\hat{P}_{(-k)}(C_i = c, S_c = s)
\right\}
$${#eq-def-cvic}

### Out-of-fold log-likelihood criterion {#sec-oofll .appendix}

We also evaluated the consistency of our cross-validation procedure
by looking at the distribution of out-of-fold log-likelihood ("OOFLL")
across cross-validation folds;
following Young et al [-@young2018uncovering],
we calculated out-of-sample log-likelihood as:

$$
\ell_{\mathcal C, k} =
\sum_{i = 1}^{n_k}
\sum_{c=1}^\mathcal C
\sum_{s\in \mathcal S}
\log
\left\{
\hat P(Y_i|C_i = c, S_c=s)
\right\}
\hat{P}_{(-k)}(C_i = c, S_c = s)
$$ {#eq-def-oof-llik}

@fig-cvic-2 shows the distribution of the OOFLL statistic
as a function of number of latent subgroups.

@fig-stage-by-age shows estimated disease progression stage
(that is, number of events experienced)
by age at visit and estimated latent subtype,
with a
Locally Estimated Scatterplot Smoothing (LOESS)
non-parametric regression curve superimposed [@cleveland1992local].
This graph is a model diagnostic; if the model fits the data well,
then older patients should on average be classified as being farther
along in their disease progression, resulting in an upwards trend.
Types 1, 3 and 4 appear to show such an upwards trend.
Type 2 has a less clear trend, but does not show substantial evidence
of lack of fit.
In all four subtypes, patients whose visit occurred at a later age
tend to be at a later estimated stage of disease progression.
The upwards trend of progression stage with age
indicates that the estimated models are plausible.

```{r}
#| label: fig-stage-by-age
#| fig-cap: |
#|    **Estimated progression stage by age and latent subtype.**
#|    The blue line is a Locally Estimated Scatterplot Smoothing (LOESS)
#|    non-parametric regression curve.
#|
#| include: true

stages =
  results_cv_max$subtype_and_stage_table |>
  mutate(
    age = patient_data$`Age at visit`,
    id = patient_data$`FXS ID`
  )

library(ggplot2)
stages |>
  graph_stage_by_age()

```

::: landscape

```{r}
#| fig-cap: "Selection criteria for number of latent subtypes"
#| fig-subcap:
#|    - "Cross-validation information criterion"
#|    - "Test set log-likelihood across folds"
#| label: fig-cvic
#| layout-ncol: 2
#| eval: !expr fit_models

temp$CVIC |> plot_CVIC()
temp$loglike_matrix |> plot_cv_loglik()
```

:::

## Additional data tables {.appendix}

The following tables summarize the distributions of the `r outcomes_name`s
that we analyzed in this paper, stratified by sex and CGG level.

::: landscape

{{< include exploration.qmd >}}

:::
